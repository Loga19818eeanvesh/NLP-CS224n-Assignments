{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_C2W4_L1_Data Preparatio.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOF8NJrJrcoigdwOtX9A5eB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Loga19818eeanvesh/Natural_Language_Processing-Assignments/blob/main/NLP_C2W4_L1_Data_Preparatio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4jpDbNGLVCO",
        "outputId": "817c2388-f890-442f-bbe1-d03022fe2b49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-1.6.3.tar.gz (174 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▉                              | 10 kB 18.8 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 20 kB 22.8 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 30 kB 27.8 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 40 kB 32.3 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 51 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 61 kB 23.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 71 kB 21.7 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 81 kB 22.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 92 kB 24.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 102 kB 21.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 112 kB 21.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 122 kB 21.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 133 kB 21.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 143 kB 21.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 153 kB 21.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 163 kB 21.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 174 kB 21.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 174 kB 21.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.6.3-py3-none-any.whl size=170298 sha256=d8d27244f24168598e153f30957efb1989caf7e28e793000e0c8744c15e783d9\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/8b/d7/ad579fbef83c287215c0caab60fb0ae0f30c4d7ce5f580eade\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-1.6.3\n"
          ]
        }
      ],
      "source": [
        "!pip install emoji\n",
        "import re\n",
        "import nltk\n",
        "import emoji\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a corpus\n",
        "corpus = 'Who,,, ❤️ \"word embeddings\"... in 2020??? I do!!!'"
      ],
      "metadata": {
        "id": "99qtNGPIMGf5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print original corpus\n",
        "print(f'Corpus:  {corpus}')\n",
        "\n",
        "# Do the substitution\n",
        "data = re.sub(r'[,!?;-]+', '.', corpus)\n",
        "\n",
        "\n",
        "# Print cleaned corpus\n",
        "print(f'After cleaning punctuation:  {data}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tt8hwLu4PmDV",
        "outputId": "3b7cdf11-10b5-4493-a633-fa36e9407fbe"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus:  Who,,, ❤️ \"word embeddings\"... in 2020??? I do!!!\n",
            "After cleaning punctuation:  Who. ❤️ \"word embeddings\"... in 2020. I do.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Print cleaned corpus\n",
        "print(f'Initial string:  {data}')\n",
        "\n",
        "# Tokenize the cleaned corpus\n",
        "data = nltk.word_tokenize(data)\n",
        "\n",
        "# Print the tokenized version of the corpus\n",
        "print(f'After tokenization:  {data}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLQ8VEaXRUNS",
        "outputId": "1d42d6dc-8eb7-48f2-a421-cb67aad376f9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Initial string:  Who. ❤️ \"word embeddings\"... in 2020. I do.\n",
            "After tokenization:  ['Who', '.', '❤️', '``', 'word', 'embeddings', \"''\", '...', 'in', '2020', '.', 'I', 'do', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the tokenized version of the corpus\n",
        "print(f'Initial list of tokens:  {data}')\n",
        "\n",
        "# Filter tokenized corpus using list comprehension\n",
        "data = [ ch.lower() for ch in data\n",
        "         if ch.isalpha()\n",
        "         or ch == '.'\n",
        "         or emoji.get_emoji_regexp().search(ch)\n",
        "       ]\n",
        "\n",
        "# Print the tokenized and filtered version of the corpus\n",
        "print(f'After cleaning:  {data}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36hsnOO9SCXy",
        "outputId": "efc35567-f97c-430c-a16e-bc8a15c0c4be"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial list of tokens:  ['Who', '.', '❤️', '``', 'word', 'embeddings', \"''\", '...', 'in', '2020', '.', 'I', 'do', '.']\n",
            "After cleaning:  ['who', '.', '❤️', 'word', 'embeddings', 'in', '.', 'i', 'do', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the 'tokenize' function that will include the steps previously seen\n",
        "def tokenize(corpus):\n",
        "    data = re.sub(r'[,!?;-]+', '.', corpus)\n",
        "    data = nltk.word_tokenize(data)  # tokenize string to words\n",
        "    data = [ ch.lower() for ch in data\n",
        "             if ch.isalpha()\n",
        "             or ch == '.'\n",
        "             or emoji.get_emoji_regexp().search(ch)\n",
        "           ]\n",
        "    return data"
      ],
      "metadata": {
        "id": "Uvb1JIdfSXk2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define new corpus\n",
        "corpus = 'Who... ❤️ \"word embeddings\",,, in 2020??? I do!!!'\n",
        "\n",
        "# Print new corpus\n",
        "print(f'Corpus:  {corpus}')\n",
        "\n",
        "# Save tokenized version of corpus into 'words' variable\n",
        "words = tokenize(corpus)\n",
        "\n",
        "# Print the tokenized version of the corpus\n",
        "print(f'Words (tokens):  {words}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mwFWgVBSfPJ",
        "outputId": "e95f02ee-679b-46ab-dbd1-facc655d5f4f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus:  Who... ❤️ \"word embeddings\",,, in 2020??? I do!!!\n",
            "Words (tokens):  ['who', '❤️', 'word', 'embeddings', '.', 'in', '.', 'i', 'do', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this with any sentence\n",
        "tokenize(\"Now it's your turn: try with your own sentence!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7NZycOOVOVq",
        "outputId": "5c9f4496-0a7f-47de-b5e6-da9f1a83a2c3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['now', 'it', 'your', 'turn', 'try', 'with', 'your', 'own', 'sentence', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the 'get_windows' function\n",
        "def get_windows(words, C):\n",
        "    i = C\n",
        "    while i < len(words) - C:\n",
        "        center_word = words[i]\n",
        "        context_words = words[(i - C):i] + words[(i+1):(i+C+1)]\n",
        "        yield context_words, center_word\n",
        "        i += 1"
      ],
      "metadata": {
        "id": "oZ-IpjK2kAT2"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print 'context_words' and 'center_word' for the new corpus with a 'context half-size' of 2\n",
        "for x, y in get_windows(['i', 'am', 'happy', 'because', 'i', 'am', 'learning'], 2):\n",
        "    print(f'{x}\\t{y}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clwGKEu1kftm",
        "outputId": "1ba7710d-4287-4005-eb0d-5598f56bc6ba"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'am', 'because', 'i']\thappy\n",
            "['am', 'happy', 'i', 'am']\tbecause\n",
            "['happy', 'because', 'am', 'learning']\ti\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dict(words):\n",
        "  words = list(set(words))\n",
        "  words.sort()\n",
        "  l = len(words)\n",
        "\n",
        "  ind = 0\n",
        "  word2ind = {}\n",
        "  ind2word = {}\n",
        "  for w in words:\n",
        "    word2ind[w] = ind\n",
        "    ind2word[ind] = w\n",
        "    ind += 1\n",
        "\n",
        "  return word2ind, ind2word\n"
      ],
      "metadata": {
        "id": "oWIdUKhxlWM1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define new corpus\n",
        "corpus = 'I am happy because I am learning'\n",
        "\n",
        "# Print new corpus\n",
        "print(f'Corpus:  {corpus}')\n",
        "\n",
        "# Save tokenized version of corpus into 'words' variable\n",
        "words = tokenize(corpus)\n",
        "\n",
        "# Get 'word2Ind' and 'Ind2word' dictionaries for the tokenized corpus\n",
        "word2Ind, Ind2word = get_dict(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGsk8xLNlT_I",
        "outputId": "50265d17-9b32-4ff5-c131-9d608d81ff61"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus:  I am happy because I am learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print 'word2Ind' dictionary\n",
        "word2Ind"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hw-39JkZmZ8z",
        "outputId": "07c417a3-e17c-4a66-e331-e77472a20d63"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'.': 0,\n",
              " 'do': 1,\n",
              " 'embeddings': 2,\n",
              " 'i': 3,\n",
              " 'in': 4,\n",
              " 'who': 5,\n",
              " 'word': 6,\n",
              " '❤️': 7}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print 'Ind2word' dictionary\n",
        "Ind2word"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkQWbcGCmleA",
        "outputId": "ea62683f-b9ad-466f-c262-0440e2231754"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: '.',\n",
              " 1: 'do',\n",
              " 2: 'embeddings',\n",
              " 3: 'i',\n",
              " 4: 'in',\n",
              " 5: 'who',\n",
              " 6: 'word',\n",
              " 7: '❤️'}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save length of word2Ind dictionary into the 'V' variable\n",
        "V = len(word2Ind)\n",
        "\n",
        "# Print length of word2Ind dictionary\n",
        "print(\"Size of vocabulary: \", V)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsE76oNXmuRk",
        "outputId": "13788429-650f-4923-b522-154d628e46b9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of vocabulary:  8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the 'word_to_one_hot_vector' function that will include the steps previously seen\n",
        "def word_to_one_hot_vector(word, word2Ind, V):\n",
        "  v = np.zeros(V)\n",
        "  n = word2Ind[word]\n",
        "  v[n]=1\n",
        "  return v"
      ],
      "metadata": {
        "id": "zv4Mb1dRm83O"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print output of 'word_to_one_hot_vector' function for word 'happy'\n",
        "word_to_one_hot_vector('.', word2Ind, V)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXTh_2WFnfUH",
        "outputId": "52738483-c88e-4b35-8294-52bca4e1530f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 0., 0., 0., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the 'context_words_to_vector' function that will include the steps previously seen\n",
        "def context_words_to_vector(context_words, word2Ind, V):\n",
        "    context_words_vectors = [word_to_one_hot_vector(w, word2Ind, V) for w in context_words]\n",
        "    context_words_vectors = np.mean(context_words_vectors, axis=0)\n",
        "    return context_words_vectors"
      ],
      "metadata": {
        "id": "W5rHkSPHoCHK"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Print output of 'context_words_to_vector' function for context words: 'am', 'happy', 'i', 'am'\n",
        "context_words_to_vector(['am', 'happy', 'i', 'am'], word2Ind, V)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNP-Fun0oGyu",
        "outputId": "fe183b05-b677-4363-ee48-4428fba17b62"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.5 , 0.  , 0.25, 0.25, 0.  , 0.  , 0.  , 0.  ])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the generator function 'get_training_example'\n",
        "def get_training_example(words, C, word2Ind, V):\n",
        "    for context_words, center_word in get_windows(words, C):\n",
        "        yield context_words_to_vector(context_words, word2Ind, V), word_to_one_hot_vector(center_word, word2Ind, V)"
      ],
      "metadata": {
        "id": "bx-WCF8Co2wX"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print vectors associated to center and context words for corpus using the generator function\n",
        "for context_words_vector, center_word_vector in get_training_example(words, 2, word2Ind, V):\n",
        "    print(f'Context words vector:  {context_words_vector}')\n",
        "    print(f'Center word vector:  {center_word_vector}')\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYwtA1svo7K9",
        "outputId": "31a8af6e-bd65-4026-9078-128143f35cf6"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context words vector:  [0.25 0.25 0.   0.5  0.   0.   0.   0.  ]\n",
            "Center word vector:  [0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "\n",
            "Context words vector:  [0.5  0.   0.25 0.25 0.   0.   0.   0.  ]\n",
            "Center word vector:  [0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "\n",
            "Context words vector:  [0.25 0.25 0.25 0.   0.25 0.   0.   0.  ]\n",
            "Center word vector:  [0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "\n"
          ]
        }
      ]
    }
  ]
}