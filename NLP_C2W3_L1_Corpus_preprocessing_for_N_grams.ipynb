{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_C2W3_L1_Corpus preprocessing for N-grams.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMYObskMmI3BYKcEapFYivV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Loga19818eeanvesh/Natural_Language_Processing-Assignments/blob/main/NLP_C2W3_L1_Corpus_preprocessing_for_N_grams.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nN6ocznD72L1",
        "outputId": "a8597b26-d0a3-4c20-c7c3-84e5b407c8f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk               # NLP toolkit\n",
        "import re                 # Library for Regular expression operations\n",
        "\n",
        "nltk.download('punkt')    # Download the Punkt sentence tokenizer "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# change the corpus to lowercase\n",
        "corpus = \"Learning% makes 'me' happy. I am happy be-cause I am learning! :)\"\n",
        "corpus = corpus.lower()\n",
        "\n",
        "# note that word \"learning\" will now be the same regardless of its position in the sentence\n",
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mMt-70G8JR9",
        "outputId": "e7028b42-96c3-42d4-a18c-3053a09e99c3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "learning% makes 'me' happy. i am happy be-cause i am learning! :)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove special characters\n",
        "corpus = \"learning% makes 'me' happy. i am happy be-cause i am learning! :)\"\n",
        "corpus = re.sub(r\"[^a-zA-Z0-9.?! ]+\", \"\", corpus)\n",
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DehZGmQH8TCt",
        "outputId": "b0bf374c-d440-4986-d3c3-a4a6f5f37293"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "learning makes me happy. i am happy because i am learning! \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split text by a delimiter to array\n",
        "input_date=\"Sat May  9 07:33:35 CEST 2020\"\n",
        "\n",
        "# get the date parts in array\n",
        "date_parts = input_date.split()\n",
        "print(f\"date parts = {date_parts}\")\n",
        "\n",
        "#get the time parts in array\n",
        "time_parts = date_parts[3].split(\":\")\n",
        "print(f\"time parts = {time_parts}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kb1Iu3d9Rem",
        "outputId": "bddea15c-ff45-477a-ae06-486d397f3c6b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "date parts = ['Sat', 'May', '9', '07:33:35', 'CEST', '2020']\n",
            "time parts = ['07', '33', '35']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize the sentence into an array of words\n",
        "\n",
        "sentence = 'i am happy because i am learning.'\n",
        "tokenized_sentence = nltk.word_tokenize(sentence)\n",
        "print(f'{sentence} -> {tokenized_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iL5fUQDw9s1Z",
        "outputId": "022e9edd-f9ee-4dfb-8e87-61fc5016e712"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i am happy because i am learning. -> ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# find length of each word in the tokenized sentence\n",
        "sentence = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n",
        "word_lengths = [(word, len(word)) for word in sentence] # Create a list with the word lengths using a list comprehension\n",
        "print(f' Lengths of the words: \\n{word_lengths}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYgtx6em911U",
        "outputId": "7ba65a10-901f-4649-ee86-1a4d36dd9fa2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Lengths of the words: \n",
            "[('i', 1), ('am', 2), ('happy', 5), ('because', 7), ('i', 1), ('am', 2), ('learning', 8), ('.', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_to_trigram(tokenized_sentence):\n",
        "    \"\"\"\n",
        "    Prints all trigrams in the given tokenized sentence.\n",
        "    \n",
        "    Args:\n",
        "        tokenized_sentence: The words list.\n",
        "    \n",
        "    Returns:\n",
        "        No output\n",
        "    \"\"\"\n",
        "\n",
        "    n = len(tokenized_sentence)\n",
        "\n",
        "    for i in range(n-2):\n",
        "      print([tokenized_sentence[i], tokenized_sentence[i+1], tokenized_sentence[i+2]])"
      ],
      "metadata": {
        "id": "cCtH6pqz-Hrv"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_sentence = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n",
        "\n",
        "print(f'List all trigrams of sentence: {tokenized_sentence}\\n')\n",
        "sentence_to_trigram(tokenized_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8bAql5b-unr",
        "outputId": "c972126f-5da6-48aa-ae44-80e81b662f04"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List all trigrams of sentence: ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n",
            "\n",
            "['i', 'am', 'happy']\n",
            "['am', 'happy', 'because']\n",
            "['happy', 'because', 'i']\n",
            "['because', 'i', 'am']\n",
            "['i', 'am', 'learning']\n",
            "['am', 'learning', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get trigram prefix from a 4-gram\n",
        "fourgram = ['i', 'am', 'happy','because']\n",
        "trigram = fourgram[0:-1] # Get the elements from 0, included, up to the last element, not included.\n",
        "print(trigram)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xU_f6HHI_fgd",
        "outputId": "5c9fee83-e8f7-4e32-aa49-9682727e9abf"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'am', 'happy']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# when working with trigrams, you need to prepend 2 <s> and append one </s>\n",
        "n = 3\n",
        "tokenized_sentence = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n",
        "tokenized_sentence = [\"<s>\"] * (n - 1) + tokenized_sentence + [\"</s>\"]\n",
        "print(tokenized_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9yCweUd_m9R",
        "outputId": "82efabe7-1327-4eb7-88cd-2012e0c14727"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<s>', '<s>', 'i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.', '</s>']\n"
          ]
        }
      ]
    }
  ]
}