{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_C1W4_A1_Word Translation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNNoPosSRiyp+jrpsjDYW1v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Loga19818eeanvesh/Natural_Language_Processing-Assignments/blob/main/NLP_C1W4_A1_Word_Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pdb\n",
        "import pickle\n",
        "import string\n",
        "\n",
        "import time\n",
        "\n",
        "import gensim\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import numpy as np\n",
        "import scipy\n",
        "import sklearn\n",
        "from gensim.models import KeyedVectors\n",
        "from nltk.corpus import stopwords, twitter_samples\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from os import getcwd\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('twitter_samples')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ScfJNzTdOB_",
        "outputId": "0caecde9-cba3-4d5a-934e-652b1a2c1916"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "\n",
        "def process_tweet(tweet):\n",
        "    '''\n",
        "    Input:\n",
        "        tweet: a string containing a tweet\n",
        "    Output:\n",
        "        tweets_clean: a list of words containing the processed tweet\n",
        "    '''\n",
        "    stemmer = PorterStemmer()\n",
        "    stopwords_english = stopwords.words('english')\n",
        "    # remove stock market tickers like $GE\n",
        "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "    # remove old style retweet text \"RT\"\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "    # remove hyperlinks\n",
        "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
        "    # remove hashtags\n",
        "    # only removing the hash # sign from the word\n",
        "    tweet = re.sub(r'#', '', tweet)\n",
        "    # tokenize tweets\n",
        "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
        "                               reduce_len=True)\n",
        "    tweet_tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "    tweets_clean = []\n",
        "    for word in tweet_tokens:\n",
        "        if (word not in stopwords_english and  # remove stopwords\n",
        "            word not in string.punctuation):  # remove punctuation\n",
        "            # tweets_clean.append(word)\n",
        "            stem_word = stemmer.stem(word)  # stemming word\n",
        "            tweets_clean.append(stem_word)\n",
        "\n",
        "    return tweets_clean\n",
        "\n",
        "\n",
        "def get_dict(file_name):\n",
        "    \"\"\"\n",
        "    This function returns the english to french dictionary given a file where the each column corresponds to a word.\n",
        "    Check out the files this function takes in your workspace.\n",
        "    \"\"\"\n",
        "    my_file = pd.read_csv(file_name, delimiter=' ')\n",
        "    etof = {}  # the english to french dictionary to be returned\n",
        "    for i in range(len(my_file)):\n",
        "        # indexing into the rows.\n",
        "        en = my_file.loc[i][0]\n",
        "        fr = my_file.loc[i][1]\n",
        "        etof[en] = fr\n",
        "\n",
        "    return etof\n",
        "\n",
        "\n",
        "def cosine_similarity(A, B):\n",
        "    '''\n",
        "    Input:\n",
        "        A: a numpy array which corresponds to a word vector\n",
        "        B: A numpy array which corresponds to a word vector\n",
        "    Output:\n",
        "        cos: numerical number representing the cosine similarity between A and B.\n",
        "    '''\n",
        "    # you have to set this variable to the true label.\n",
        "    cos = -10\n",
        "    dot = np.dot(A, B)\n",
        "    norma = np.linalg.norm(A)\n",
        "    normb = np.linalg.norm(B)\n",
        "    cos = dot / (norma * normb)\n",
        "\n",
        "    return cos"
      ],
      "metadata": {
        "id": "17_4D0T4ddkp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_embeddings_subset = pickle.load(open(\"en_embeddings.p\", \"rb\"))\n",
        "fr_embeddings_subset = pickle.load(open(\"fr_embeddings.p\", \"rb\"))\n"
      ],
      "metadata": {
        "id": "MDXUoSntd6Jc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the english to french dictionaries\n",
        "en_fr_train = get_dict('en-fr.train.txt')\n",
        "print('The length of the English to French training dictionary is', len(en_fr_train))\n",
        "en_fr_test = get_dict('en-fr.test.txt')\n",
        "print('The length of the English to French test dictionary is', len(en_fr_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UORsdFOddSyO",
        "outputId": "2d71858f-8000-40c0-fa05-a567dfbc702d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The length of the English to French training dictionary is 5000\n",
            "The length of the English to French test dictionary is 5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "def get_matrices(en_fr, french_vecs, english_vecs):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        en_fr: English to French dictionary\n",
        "        french_vecs: French words to their corresponding word embeddings.\n",
        "        english_vecs: English words to their corresponding word embeddings.\n",
        "    Output: \n",
        "        X: a matrix where the columns are the English embeddings.\n",
        "        Y: a matrix where the columns correspong to the French embeddings.\n",
        "        R: the projection matrix that minimizes the F norm ||X R -Y||^2.\n",
        "    \"\"\"\n",
        "    x = []\n",
        "    y = []\n",
        "    for en, fr in en_fr.items():\n",
        "      if en in english_vecs and fr in french_vecs:\n",
        "        x.append(english_vecs[en])\n",
        "        y.append(french_vecs[fr])\n",
        "\n",
        "    x = np.array(x)\n",
        "    y = np.array(y)\n",
        "\n",
        "    return x,y"
      ],
      "metadata": {
        "id": "dyC8ScUdgFk8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n",
        "\n",
        "# getting the training set:\n",
        "X_train, Y_train = get_matrices(\n",
        "    en_fr_train, fr_embeddings_subset, en_embeddings_subset)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(Y_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8R-h9z_heRD",
        "outputId": "fabbf772-a56e-45a7-fffa-0a9f2bbf9377"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4932, 300)\n",
            "(4932, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(X, Y, R):\n",
        "    '''\n",
        "    Inputs: \n",
        "        X: a matrix of dimension (m,n) where the columns are the English embeddings.\n",
        "        Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.\n",
        "        R: a matrix of dimension (n,n) - transformation matrix from English to French vector space embeddings.\n",
        "    Outputs:\n",
        "        L: a matrix of dimension (m,n) - the value of the loss function for given X, Y and R.\n",
        "    '''\n",
        "    m = X.shape[0]\n",
        "    L=np.dot(X,R) - Y\n",
        "    L = L*L\n",
        "    loss = np.sum(L)/m\n",
        "\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "E34VqdcpiuTs"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient(X, Y, R):\n",
        "    '''\n",
        "    Inputs: \n",
        "        X: a matrix of dimension (m,n) where the columns are the English embeddings.\n",
        "        Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.\n",
        "        R: a matrix of dimension (n,n) - transformation matrix from English to French vector space embeddings.\n",
        "    Outputs:\n",
        "        g: a matrix of dimension (n,n) - gradient of the loss function L for given X, Y and R.\n",
        "    '''\n",
        "    m = X.shape[0]\n",
        "    L=np.dot(X,R) - Y\n",
        "    G = np.dot(X.T, L)*(2/m)\n",
        "\n",
        "    return G\n"
      ],
      "metadata": {
        "id": "VVrnT2N3j2gS"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def align_embeddings(X, Y, train_steps=100, learning_rate=0.0003):\n",
        "    '''\n",
        "    Inputs:\n",
        "        X: a matrix of dimension (m,n) where the columns are the English embeddings.\n",
        "        Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.\n",
        "        train_steps: positive int - describes how many steps will gradient descent algorithm do.\n",
        "        learning_rate: positive float - describes how big steps will  gradient descent algorithm do.\n",
        "    Outputs:\n",
        "        R: a matrix of dimension (n,n) - the projection matrix that minimizes the F norm ||X R -Y||^2\n",
        "    '''\n",
        "\n",
        "    n = m = X.shape[1]\n",
        "\n",
        "    R = np.random.rand(n,n)\n",
        "\n",
        "    for i in range(train_steps) :\n",
        "      R = R - learning_rate*compute_gradient(X, Y, R)\n",
        "      loss = compute_loss(X,Y,R)\n",
        "      print(\"loss at\", i , \"th iteration is\", loss)\n",
        "    return R \n",
        "    "
      ],
      "metadata": {
        "id": "StN_6WVHlJZ4"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(129)\n",
        "m = 10\n",
        "n = 5\n",
        "X = np.random.rand(m, n)\n",
        "Y = np.random.rand(m, n) * .1\n",
        "R = align_embeddings(X, Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EV_WCSpNmZkY",
        "outputId": "6fca83d1-41f5-4c67-bb21-7290983d7f59"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss at 0 th iteration is 3.466034627663061\n",
            "loss at 1 th iteration is 3.462386009856361\n",
            "loss at 2 th iteration is 3.4587413412930914\n",
            "loss at 3 th iteration is 3.4551006176821017\n",
            "loss at 4 th iteration is 3.4514638347369058\n",
            "loss at 5 th iteration is 3.447830988175678\n",
            "loss at 6 th iteration is 3.4442020737212466\n",
            "loss at 7 th iteration is 3.4405770871010946\n",
            "loss at 8 th iteration is 3.436956024047345\n",
            "loss at 9 th iteration is 3.433338880296765\n",
            "loss at 10 th iteration is 3.4297256515907533\n",
            "loss at 11 th iteration is 3.4261163336753433\n",
            "loss at 12 th iteration is 3.4225109223011883\n",
            "loss at 13 th iteration is 3.418909413223568\n",
            "loss at 14 th iteration is 3.415311802202372\n",
            "loss at 15 th iteration is 3.4117180850021014\n",
            "loss at 16 th iteration is 3.4081282573918608\n",
            "loss at 17 th iteration is 3.404542315145359\n",
            "loss at 18 th iteration is 3.400960254040897\n",
            "loss at 19 th iteration is 3.397382069861364\n",
            "loss at 20 th iteration is 3.3938077583942396\n",
            "loss at 21 th iteration is 3.390237315431577\n",
            "loss at 22 th iteration is 3.3866707367700095\n",
            "loss at 23 th iteration is 3.3831080182107387\n",
            "loss at 24 th iteration is 3.3795491555595296\n",
            "loss at 25 th iteration is 3.3759941446267105\n",
            "loss at 26 th iteration is 3.372442981227164\n",
            "loss at 27 th iteration is 3.368895661180319\n",
            "loss at 28 th iteration is 3.3653521803101567\n",
            "loss at 29 th iteration is 3.3618125344451935\n",
            "loss at 30 th iteration is 3.358276719418483\n",
            "loss at 31 th iteration is 3.3547447310676075\n",
            "loss at 32 th iteration is 3.3512165652346786\n",
            "loss at 33 th iteration is 3.347692217766324\n",
            "loss at 34 th iteration is 3.3441716845136917\n",
            "loss at 35 th iteration is 3.3406549613324357\n",
            "loss at 36 th iteration is 3.3371420440827206\n",
            "loss at 37 th iteration is 3.3336329286292083\n",
            "loss at 38 th iteration is 3.330127610841059\n",
            "loss at 39 th iteration is 3.3266260865919235\n",
            "loss at 40 th iteration is 3.3231283517599395\n",
            "loss at 41 th iteration is 3.319634402227723\n",
            "loss at 42 th iteration is 3.3161442338823712\n",
            "loss at 43 th iteration is 3.3126578426154496\n",
            "loss at 44 th iteration is 3.3091752243229955\n",
            "loss at 45 th iteration is 3.3056963749054993\n",
            "loss at 46 th iteration is 3.3022212902679167\n",
            "loss at 47 th iteration is 3.298749966319653\n",
            "loss at 48 th iteration is 3.2952823989745594\n",
            "loss at 49 th iteration is 3.291818584150932\n",
            "loss at 50 th iteration is 3.288358517771504\n",
            "loss at 51 th iteration is 3.284902195763441\n",
            "loss at 52 th iteration is 3.281449614058338\n",
            "loss at 53 th iteration is 3.27800076859221\n",
            "loss at 54 th iteration is 3.2745556553054955\n",
            "loss at 55 th iteration is 3.271114270143044\n",
            "loss at 56 th iteration is 3.2676766090541123\n",
            "loss at 57 th iteration is 3.2642426679923653\n",
            "loss at 58 th iteration is 3.2608124429158623\n",
            "loss at 59 th iteration is 3.2573859297870627\n",
            "loss at 60 th iteration is 3.253963124572812\n",
            "loss at 61 th iteration is 3.2505440232443403\n",
            "loss at 62 th iteration is 3.2471286217772617\n",
            "loss at 63 th iteration is 3.2437169161515613\n",
            "loss at 64 th iteration is 3.2403089023515976\n",
            "loss at 65 th iteration is 3.2369045763660935\n",
            "loss at 66 th iteration is 3.2335039341881355\n",
            "loss at 67 th iteration is 3.2301069718151636\n",
            "loss at 68 th iteration is 3.226713685248973\n",
            "loss at 69 th iteration is 3.223324070495702\n",
            "loss at 70 th iteration is 3.2199381235658335\n",
            "loss at 71 th iteration is 3.216555840474188\n",
            "loss at 72 th iteration is 3.213177217239918\n",
            "loss at 73 th iteration is 3.209802249886505\n",
            "loss at 74 th iteration is 3.2064309344417516\n",
            "loss at 75 th iteration is 3.2030632669377823\n",
            "loss at 76 th iteration is 3.1996992434110347\n",
            "loss at 77 th iteration is 3.196338859902254\n",
            "loss at 78 th iteration is 3.192982112456493\n",
            "loss at 79 th iteration is 3.1896289971231013\n",
            "loss at 80 th iteration is 3.1862795099557246\n",
            "loss at 81 th iteration is 3.1829336470123017\n",
            "loss at 82 th iteration is 3.1795914043550555\n",
            "loss at 83 th iteration is 3.1762527780504892\n",
            "loss at 84 th iteration is 3.1729177641693838\n",
            "loss at 85 th iteration is 3.169586358786792\n",
            "loss at 86 th iteration is 3.166258557982035\n",
            "loss at 87 th iteration is 3.1629343578386937\n",
            "loss at 88 th iteration is 3.159613754444612\n",
            "loss at 89 th iteration is 3.1562967438918816\n",
            "loss at 90 th iteration is 3.152983322276847\n",
            "loss at 91 th iteration is 3.1496734857000943\n",
            "loss at 92 th iteration is 3.1463672302664523\n",
            "loss at 93 th iteration is 3.143064552084982\n",
            "loss at 94 th iteration is 3.1397654472689758\n",
            "loss at 95 th iteration is 3.136469911935952\n",
            "loss at 96 th iteration is 3.133177942207649\n",
            "loss at 97 th iteration is 3.1298895342100237\n",
            "loss at 98 th iteration is 3.1266046840732424\n",
            "loss at 99 th iteration is 3.123323387931683\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# UNQ_C7 (UNIQU\n",
        "#E CELL IDENTIFIER, DO NOT EDIT)\n",
        "# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n",
        "R_train = align_embeddings(X_train, Y_train, train_steps=400, learning_rate=0.8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8DR4sYJm6fd",
        "outputId": "74833c24-9f5d-45f5-aa09-890727e6a913"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss at 0 th iteration is 692.1378679372747\n",
            "loss at 1 th iteration is 602.5429548738636\n",
            "loss at 2 th iteration is 533.0304624393692\n",
            "loss at 3 th iteration is 476.4432601574037\n",
            "loss at 4 th iteration is 429.2364019588533\n",
            "loss at 5 th iteration is 389.13958039664044\n",
            "loss at 6 th iteration is 354.5843627961007\n",
            "loss at 7 th iteration is 324.45022082334566\n",
            "loss at 8 th iteration is 297.91609621427045\n",
            "loss at 9 th iteration is 274.3660550261841\n",
            "loss at 10 th iteration is 253.3275668290338\n",
            "loss at 11 th iteration is 234.43041347457387\n",
            "loss at 12 th iteration is 217.37889064362713\n",
            "loss at 13 th iteration is 201.9326894823294\n",
            "loss at 14 th iteration is 187.89351910056232\n",
            "loss at 15 th iteration is 175.0955767846215\n",
            "loss at 16 th iteration is 163.39863358760996\n",
            "loss at 17 th iteration is 152.68292426101064\n",
            "loss at 18 th iteration is 142.84530155676552\n",
            "loss at 19 th iteration is 133.79629097800253\n",
            "loss at 20 th iteration is 125.45779751560846\n",
            "loss at 21 th iteration is 117.76129240801278\n",
            "loss at 22 th iteration is 110.64635919719663\n",
            "loss at 23 th iteration is 104.05951306118449\n",
            "loss at 24 th iteration is 97.95323119799205\n",
            "loss at 25 th iteration is 92.2851485565791\n",
            "loss at 26 th iteration is 87.01738483285823\n",
            "loss at 27 th iteration is 82.11597693788488\n",
            "loss at 28 th iteration is 77.55039714017359\n",
            "loss at 29 th iteration is 73.29314148114676\n",
            "loss at 30 th iteration is 69.31937633328646\n",
            "loss at 31 th iteration is 65.60663343671828\n",
            "loss at 32 th iteration is 62.1345456341977\n",
            "loss at 33 th iteration is 58.884616982193286\n",
            "loss at 34 th iteration is 55.84002205677064\n",
            "loss at 35 th iteration is 52.98543017579813\n",
            "loss at 36 th iteration is 50.30685098045782\n",
            "loss at 37 th iteration is 47.79149840080315\n",
            "loss at 38 th iteration is 45.427670503051345\n",
            "loss at 39 th iteration is 43.20464310362973\n",
            "loss at 40 th iteration is 41.11257535431779\n",
            "loss at 41 th iteration is 39.14242576766886\n",
            "loss at 42 th iteration is 37.2858773727392\n",
            "loss at 43 th iteration is 35.53527087622515\n",
            "loss at 44 th iteration is 33.883544859892176\n",
            "loss at 45 th iteration is 32.32418217686159\n",
            "loss at 46 th iteration is 30.8511618210433\n",
            "loss at 47 th iteration is 29.458915639135174\n",
            "loss at 48 th iteration is 28.142289335875233\n",
            "loss at 49 th iteration is 26.896507292869206\n",
            "loss at 50 th iteration is 25.717140781164925\n",
            "loss at 51 th iteration is 24.600079199313246\n",
            "loss at 52 th iteration is 23.541504013216798\n",
            "loss at 53 th iteration is 22.53786511265833\n",
            "loss at 54 th iteration is 21.585859332908594\n",
            "loss at 55 th iteration is 20.68241091896713\n",
            "loss at 56 th iteration is 19.8246537354156\n",
            "loss at 57 th iteration is 19.009915047078856\n",
            "loss at 58 th iteration is 18.235700715143793\n",
            "loss at 59 th iteration is 17.49968167045171\n",
            "loss at 60 th iteration is 16.799681540679952\n",
            "loss at 61 th iteration is 16.133665321337055\n",
            "loss at 62 th iteration is 15.49972899214393\n",
            "loss at 63 th iteration is 14.896089990666995\n",
            "loss at 64 th iteration is 14.32107846417667\n",
            "loss at 65 th iteration is 13.773129228777941\n",
            "loss at 66 th iteration is 13.250774372025013\n",
            "loss at 67 th iteration is 12.752636441603373\n",
            "loss at 68 th iteration is 12.277422168333146\n",
            "loss at 69 th iteration is 11.823916676803927\n",
            "loss at 70 th iteration is 11.3909781414646\n",
            "loss at 71 th iteration is 10.97753285002587\n",
            "loss at 72 th iteration is 10.582570639644034\n",
            "loss at 73 th iteration is 10.205140674590009\n",
            "loss at 74 th iteration is 9.844347537010773\n",
            "loss at 75 th iteration is 9.499347604997588\n",
            "loss at 76 th iteration is 9.169345694520729\n",
            "loss at 77 th iteration is 8.853591943900966\n",
            "loss at 78 th iteration is 8.551378921391324\n",
            "loss at 79 th iteration is 8.262038938158524\n",
            "loss at 80 th iteration is 7.984941550504471\n",
            "loss at 81 th iteration is 7.719491236569457\n",
            "loss at 82 th iteration is 7.4651252340272425\n",
            "loss at 83 th iteration is 7.221311526431308\n",
            "loss at 84 th iteration is 6.98754696691327\n",
            "loss at 85 th iteration is 6.763355528880163\n",
            "loss at 86 th iteration is 6.5482866742159285\n",
            "loss at 87 th iteration is 6.341913830273648\n",
            "loss at 88 th iteration is 6.143832967655488\n",
            "loss at 89 th iteration is 5.953661271424539\n",
            "loss at 90 th iteration is 5.771035898982823\n",
            "loss at 91 th iteration is 5.5956128183877\n",
            "loss at 92 th iteration is 5.427065721370523\n",
            "loss at 93 th iteration is 5.265085005770246\n",
            "loss at 94 th iteration is 5.109376822505288\n",
            "loss at 95 th iteration is 4.959662182582714\n",
            "loss at 96 th iteration is 4.815676119987897\n",
            "loss at 97 th iteration is 4.6771669066132135\n",
            "loss at 98 th iteration is 4.5438953156735495\n",
            "loss at 99 th iteration is 4.4156339303218415\n",
            "loss at 100 th iteration is 4.292166494421826\n",
            "loss at 101 th iteration is 4.17328730265905\n",
            "loss at 102 th iteration is 4.058800627377481\n",
            "loss at 103 th iteration is 3.9485201797186154\n",
            "loss at 104 th iteration is 3.8422686028146233\n",
            "loss at 105 th iteration is 3.7398769949480557\n",
            "loss at 106 th iteration is 3.6411844607390194\n",
            "loss at 107 th iteration is 3.546037688557569\n",
            "loss at 108 th iteration is 3.4542905524855407\n",
            "loss at 109 th iteration is 3.3658037372686813\n",
            "loss at 110 th iteration is 3.2804443848079177\n",
            "loss at 111 th iteration is 3.198085760838311\n",
            "loss at 112 th iteration is 3.118606940536614\n",
            "loss at 113 th iteration is 3.0418925118837383\n",
            "loss at 114 th iteration is 2.967832295687674\n",
            "loss at 115 th iteration is 2.896321081245775\n",
            "loss at 116 th iteration is 2.82725837669325\n",
            "loss at 117 th iteration is 2.7605481731479533\n",
            "loss at 118 th iteration is 2.696098721820007\n",
            "loss at 119 th iteration is 2.633822323309332\n",
            "loss at 120 th iteration is 2.5736351283645624\n",
            "loss at 121 th iteration is 2.51545694942391\n",
            "loss at 122 th iteration is 2.4592110823021778\n",
            "loss at 123 th iteration is 2.404824137428798\n",
            "loss at 124 th iteration is 2.3522258800795863\n",
            "loss at 125 th iteration is 2.301349079080179\n",
            "loss at 126 th iteration is 2.25212936349187\n",
            "loss at 127 th iteration is 2.2045050868213023\n",
            "loss at 128 th iteration is 2.158417198323794\n",
            "loss at 129 th iteration is 2.113809120996928\n",
            "loss at 130 th iteration is 2.0706266358856547\n",
            "loss at 131 th iteration is 2.0288177723434844\n",
            "loss at 132 th iteration is 1.9883327039159566\n",
            "loss at 133 th iteration is 1.9491236495327868\n",
            "loss at 134 th iteration is 1.9111447797140093\n",
            "loss at 135 th iteration is 1.8743521275131543\n",
            "loss at 136 th iteration is 1.8387035039369888\n",
            "loss at 137 th iteration is 1.8041584175968197\n",
            "loss at 138 th iteration is 1.770677998360919\n",
            "loss at 139 th iteration is 1.738224924791072\n",
            "loss at 140 th iteration is 1.7067633551590862\n",
            "loss at 141 th iteration is 1.6762588618508873\n",
            "loss at 142 th iteration is 1.6466783689770674\n",
            "loss at 143 th iteration is 1.6179900930191677\n",
            "loss at 144 th iteration is 1.5901634863508105\n",
            "loss at 145 th iteration is 1.5631691834819827\n",
            "loss at 146 th iteration is 1.5369789498834587\n",
            "loss at 147 th iteration is 1.5115656332563785\n",
            "loss at 148 th iteration is 1.4869031171197187\n",
            "loss at 149 th iteration is 1.4629662765954587\n",
            "loss at 150 th iteration is 1.4397309362780297\n",
            "loss at 151 th iteration is 1.4171738300809118\n",
            "loss at 152 th iteration is 1.3952725629592246\n",
            "loss at 153 th iteration is 1.374005574412724\n",
            "loss at 154 th iteration is 1.3533521036788625\n",
            "loss at 155 th iteration is 1.333292156530558\n",
            "loss at 156 th iteration is 1.3138064735979318\n",
            "loss at 157 th iteration is 1.2948765001376747\n",
            "loss at 158 th iteration is 1.2764843571778604\n",
            "loss at 159 th iteration is 1.2586128139698387\n",
            "loss at 160 th iteration is 1.2412452616826015\n",
            "loss at 161 th iteration is 1.2243656882783922\n",
            "loss at 162 th iteration is 1.2079586545116174\n",
            "loss at 163 th iteration is 1.192009270996196\n",
            "loss at 164 th iteration is 1.1765031762893672\n",
            "loss at 165 th iteration is 1.1614265159426909\n",
            "loss at 166 th iteration is 1.1467659224736135\n",
            "loss at 167 th iteration is 1.132508496213326\n",
            "loss at 168 th iteration is 1.118641786988997\n",
            "loss at 169 th iteration is 1.1051537766006188\n",
            "loss at 170 th iteration is 1.0920328620547364\n",
            "loss at 171 th iteration is 1.079267839519299\n",
            "loss at 172 th iteration is 1.066847888965651\n",
            "loss at 173 th iteration is 1.0547625594654766\n",
            "loss at 174 th iteration is 1.043001755112091\n",
            "loss at 175 th iteration is 1.0315557215370394\n",
            "loss at 176 th iteration is 1.020415032994429\n",
            "loss at 177 th iteration is 1.009570579986798\n",
            "loss at 178 th iteration is 0.9990135574076379\n",
            "loss at 179 th iteration is 0.988735453176913\n",
            "loss at 180 th iteration is 0.978728037347112\n",
            "loss at 181 th iteration is 0.9689833516584573\n",
            "loss at 182 th iteration is 0.959493699522976\n",
            "loss at 183 th iteration is 0.950251636418081\n",
            "loss at 184 th iteration is 0.941249960671335\n",
            "loss at 185 th iteration is 0.9324817046188709\n",
            "loss at 186 th iteration is 0.9239401261208677\n",
            "loss at 187 th iteration is 0.9156187004182416\n",
            "loss at 188 th iteration is 0.9075111123154753\n",
            "loss at 189 th iteration is 0.8996112486752602\n",
            "loss at 190 th iteration is 0.8919131912112683\n",
            "loss at 191 th iteration is 0.8844112095660644\n",
            "loss at 192 th iteration is 0.8770997546617393\n",
            "loss at 193 th iteration is 0.8699734523114838\n",
            "loss at 194 th iteration is 0.8630270970808276\n",
            "loss at 195 th iteration is 0.8562556463878297\n",
            "loss at 196 th iteration is 0.8496542148319854\n",
            "loss at 197 th iteration is 0.8432180687421122\n",
            "loss at 198 th iteration is 0.8369426209339089\n",
            "loss at 199 th iteration is 0.8308234256683226\n",
            "loss at 200 th iteration is 0.8248561738022661\n",
            "loss at 201 th iteration is 0.8190366881236036\n",
            "loss at 202 th iteration is 0.8133609188627225\n",
            "loss at 203 th iteration is 0.807824939373309\n",
            "loss at 204 th iteration is 0.8024249419753391\n",
            "loss at 205 th iteration is 0.7971572339535566\n",
            "loss at 206 th iteration is 0.792018233705056\n",
            "loss at 207 th iteration is 0.7870044670298437\n",
            "loss at 208 th iteration is 0.7821125635585495\n",
            "loss at 209 th iteration is 0.7773392533116941\n",
            "loss at 210 th iteration is 0.7726813633851908\n",
            "loss at 211 th iteration is 0.7681358147569837\n",
            "loss at 212 th iteration is 0.7636996192099358\n",
            "loss at 213 th iteration is 0.7593698763663329\n",
            "loss at 214 th iteration is 0.7551437708295204\n",
            "loss at 215 th iteration is 0.7510185694284325\n",
            "loss at 216 th iteration is 0.7469916185609272\n",
            "loss at 217 th iteration is 0.7430603416320323\n",
            "loss at 218 th iteration is 0.7392222365833635\n",
            "loss at 219 th iteration is 0.7354748735101494\n",
            "loss at 220 th iteration is 0.7318158923624443\n",
            "loss at 221 th iteration is 0.7282430007272481\n",
            "loss at 222 th iteration is 0.724753971688407\n",
            "loss at 223 th iteration is 0.721346641761291\n",
            "loss at 224 th iteration is 0.718018908899367\n",
            "loss at 225 th iteration is 0.7147687305699254\n",
            "loss at 226 th iteration is 0.7115941218963064\n",
            "loss at 227 th iteration is 0.7084931538641108\n",
            "loss at 228 th iteration is 0.705463951588961\n",
            "loss at 229 th iteration is 0.7025046926434942\n",
            "loss at 230 th iteration is 0.6996136054413516\n",
            "loss at 231 th iteration is 0.696788967676048\n",
            "loss at 232 th iteration is 0.6940291048126465\n",
            "loss at 233 th iteration is 0.691332388630288\n",
            "loss at 234 th iteration is 0.6886972358136951\n",
            "loss at 235 th iteration is 0.6861221065918279\n",
            "loss at 236 th iteration is 0.6836055034219668\n",
            "loss at 237 th iteration is 0.6811459697175489\n",
            "loss at 238 th iteration is 0.6787420886181664\n",
            "loss at 239 th iteration is 0.6763924818001913\n",
            "loss at 240 th iteration is 0.6740958083265368\n",
            "loss at 241 th iteration is 0.6718507635341685\n",
            "loss at 242 th iteration is 0.6696560779579777\n",
            "loss at 243 th iteration is 0.6675105162897282\n",
            "loss at 244 th iteration is 0.6654128763708168\n",
            "loss at 245 th iteration is 0.6633619882176395\n",
            "loss at 246 th iteration is 0.6613567130784073\n",
            "loss at 247 th iteration is 0.6593959425202991\n",
            "loss at 248 th iteration is 0.6574785975458797\n",
            "loss at 249 th iteration is 0.6556036277377523\n",
            "loss at 250 th iteration is 0.6537700104304569\n",
            "loss at 251 th iteration is 0.6519767499086673\n",
            "loss at 252 th iteration is 0.6502228766307615\n",
            "loss at 253 th iteration is 0.6485074464768996\n",
            "loss at 254 th iteration is 0.6468295400207411\n",
            "loss at 255 th iteration is 0.6451882618240128\n",
            "loss at 256 th iteration is 0.643582739753116\n",
            "loss at 257 th iteration is 0.6420121243170444\n",
            "loss at 258 th iteration is 0.6404755880258571\n",
            "loss at 259 th iteration is 0.6389723247690364\n",
            "loss at 260 th iteration is 0.6375015492130369\n",
            "loss at 261 th iteration is 0.6360624962173832\n",
            "loss at 262 th iteration is 0.6346544202686921\n",
            "loss at 263 th iteration is 0.6332765949320179\n",
            "loss at 264 th iteration is 0.6319283123189405\n",
            "loss at 265 th iteration is 0.6306088825718367\n",
            "loss at 266 th iteration is 0.6293176333637993\n",
            "loss at 267 th iteration is 0.6280539094136786\n",
            "loss at 268 th iteration is 0.6268170720157593\n",
            "loss at 269 th iteration is 0.6256064985835792\n",
            "loss at 270 th iteration is 0.6244215822074244\n",
            "loss at 271 th iteration is 0.6232617312250686\n",
            "loss at 272 th iteration is 0.6221263688053041\n",
            "loss at 273 th iteration is 0.6210149325438653\n",
            "loss at 274 th iteration is 0.6199268740713278\n",
            "loss at 275 th iteration is 0.6188616586726121\n",
            "loss at 276 th iteration is 0.6178187649177036\n",
            "loss at 277 th iteration is 0.6167976843032358\n",
            "loss at 278 th iteration is 0.615797920904586\n",
            "loss at 279 th iteration is 0.6148189910381512\n",
            "loss at 280 th iteration is 0.6138604229334736\n",
            "loss at 281 th iteration is 0.612921756414908\n",
            "loss at 282 th iteration is 0.6120025425925306\n",
            "loss at 283 th iteration is 0.6111023435619868\n",
            "loss at 284 th iteration is 0.6102207321130106\n",
            "loss at 285 th iteration is 0.6093572914463365\n",
            "loss at 286 th iteration is 0.6085116148987361\n",
            "loss at 287 th iteration is 0.6076833056759344\n",
            "loss at 288 th iteration is 0.6068719765931606\n",
            "loss at 289 th iteration is 0.6060772498230872\n",
            "loss at 290 th iteration is 0.6052987566509365\n",
            "loss at 291 th iteration is 0.6045361372365352\n",
            "loss at 292 th iteration is 0.6037890403830933\n",
            "loss at 293 th iteration is 0.6030571233125105\n",
            "loss at 294 th iteration is 0.6023400514470086\n",
            "loss at 295 th iteration is 0.6016374981968888\n",
            "loss at 296 th iteration is 0.600949144754242\n",
            "loss at 297 th iteration is 0.6002746798924153\n",
            "loss at 298 th iteration is 0.5996137997710722\n",
            "loss at 299 th iteration is 0.5989662077466736\n",
            "loss at 300 th iteration is 0.5983316141882147\n",
            "loss at 301 th iteration is 0.5977097362980628\n",
            "loss at 302 th iteration is 0.5971002979377388\n",
            "loss at 303 th iteration is 0.5965030294585015\n",
            "loss at 304 th iteration is 0.5959176675365876\n",
            "loss at 305 th iteration is 0.5953439550129634\n",
            "loss at 306 th iteration is 0.5947816407374694\n",
            "loss at 307 th iteration is 0.594230479417209\n",
            "loss at 308 th iteration is 0.5936902314690722\n",
            "loss at 309 th iteration is 0.5931606628762658\n",
            "loss at 310 th iteration is 0.5926415450487265\n",
            "loss at 311 th iteration is 0.5921326546873193\n",
            "loss at 312 th iteration is 0.5916337736516922\n",
            "loss at 313 th iteration is 0.5911446888316932\n",
            "loss at 314 th iteration is 0.5906651920222438\n",
            "loss at 315 th iteration is 0.5901950798015614\n",
            "loss at 316 th iteration is 0.5897341534126487\n",
            "loss at 317 th iteration is 0.5892822186479336\n",
            "loss at 318 th iteration is 0.5888390857369975\n",
            "loss at 319 th iteration is 0.5884045692372729\n",
            "loss at 320 th iteration is 0.5879784879276525\n",
            "loss at 321 th iteration is 0.5875606647049039\n",
            "loss at 322 th iteration is 0.5871509264828288\n",
            "loss at 323 th iteration is 0.5867491040940725\n",
            "loss at 324 th iteration is 0.586355032194523\n",
            "loss at 325 th iteration is 0.5859685491702135\n",
            "loss at 326 th iteration is 0.5855894970466669\n",
            "loss at 327 th iteration is 0.5852177214006093\n",
            "loss at 328 th iteration is 0.5848530712739852\n",
            "loss at 329 th iteration is 0.5844953990902124\n",
            "loss at 330 th iteration is 0.5841445605726148\n",
            "loss at 331 th iteration is 0.583800414664964\n",
            "loss at 332 th iteration is 0.5834628234540848\n",
            "loss at 333 th iteration is 0.5831316520944507\n",
            "loss at 334 th iteration is 0.5828067687347366\n",
            "loss at 335 th iteration is 0.5824880444462398\n",
            "loss at 336 th iteration is 0.5821753531531636\n",
            "loss at 337 th iteration is 0.5818685715646653\n",
            "loss at 338 th iteration is 0.5815675791086568\n",
            "loss at 339 th iteration is 0.581272257867286\n",
            "loss at 340 th iteration is 0.5809824925140693\n",
            "loss at 341 th iteration is 0.5806981702526143\n",
            "loss at 342 th iteration is 0.5804191807569059\n",
            "loss at 343 th iteration is 0.5801454161130989\n",
            "loss at 344 th iteration is 0.5798767707627855\n",
            "loss at 345 th iteration is 0.5796131414476933\n",
            "loss at 346 th iteration is 0.5793544271557758\n",
            "loss at 347 th iteration is 0.57910052906866\n",
            "loss at 348 th iteration is 0.5788513505104103\n",
            "loss at 349 th iteration is 0.5786067968975808\n",
            "loss at 350 th iteration is 0.5783667756905059\n",
            "loss at 351 th iteration is 0.578131196345821\n",
            "loss at 352 th iteration is 0.5778999702701548\n",
            "loss at 353 th iteration is 0.5776730107749766\n",
            "loss at 354 th iteration is 0.5774502330325649\n",
            "loss at 355 th iteration is 0.5772315540330697\n",
            "loss at 356 th iteration is 0.5770168925426271\n",
            "loss at 357 th iteration is 0.5768061690625218\n",
            "loss at 358 th iteration is 0.576599305789346\n",
            "loss at 359 th iteration is 0.576396226576138\n",
            "loss at 360 th iteration is 0.5761968568944836\n",
            "loss at 361 th iteration is 0.5760011237975413\n",
            "loss at 362 th iteration is 0.5758089558839667\n",
            "loss at 363 th iteration is 0.5756202832627316\n",
            "loss at 364 th iteration is 0.5754350375187877\n",
            "loss at 365 th iteration is 0.5752531516795772\n",
            "loss at 366 th iteration is 0.5750745601823467\n",
            "loss at 367 th iteration is 0.5748991988422646\n",
            "loss at 368 th iteration is 0.5747270048213016\n",
            "loss at 369 th iteration is 0.5745579165978718\n",
            "loss at 370 th iteration is 0.5743918739371985\n",
            "loss at 371 th iteration is 0.5742288178624025\n",
            "loss at 372 th iteration is 0.5740686906262776\n",
            "loss at 373 th iteration is 0.5739114356837501\n",
            "loss at 374 th iteration is 0.5737569976649972\n",
            "loss at 375 th iteration is 0.5736053223492069\n",
            "loss at 376 th iteration is 0.5734563566389652\n",
            "loss at 377 th iteration is 0.5733100485352585\n",
            "loss at 378 th iteration is 0.5731663471130655\n",
            "loss at 379 th iteration is 0.5730252024975332\n",
            "loss at 380 th iteration is 0.5728865658407194\n",
            "loss at 381 th iteration is 0.5727503892988798\n",
            "loss at 382 th iteration is 0.5726166260103038\n",
            "loss at 383 th iteration is 0.5724852300736616\n",
            "loss at 384 th iteration is 0.5723561565268708\n",
            "loss at 385 th iteration is 0.5722293613264581\n",
            "loss at 386 th iteration is 0.5721048013274037\n",
            "loss at 387 th iteration is 0.5719824342634632\n",
            "loss at 388 th iteration is 0.5718622187279455\n",
            "loss at 389 th iteration is 0.5717441141549477\n",
            "loss at 390 th iteration is 0.5716280808010185\n",
            "loss at 391 th iteration is 0.5715140797272598\n",
            "loss at 392 th iteration is 0.5714020727818352\n",
            "loss at 393 th iteration is 0.5712920225828956\n",
            "loss at 394 th iteration is 0.5711838925018902\n",
            "loss at 395 th iteration is 0.5710776466472722\n",
            "loss at 396 th iteration is 0.5709732498485797\n",
            "loss at 397 th iteration is 0.5708706676408802\n",
            "loss at 398 th iteration is 0.5707698662495833\n",
            "loss at 399 th iteration is 0.5706708125755913\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def nearest_neighbor1(v, candidates, k=1):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "      - v, the vector you are going find the nearest neighbor for\n",
        "      - candidates: a set of vectors where we will find the neighbors\n",
        "      - k: top k nearest neighbors to find\n",
        "    Output:\n",
        "      - k_idx: the indices of the top k closest vectors in sorted form\n",
        "    \"\"\"\n",
        "    s = []\n",
        "    for row in candidates : \n",
        "      s.append(cosine_similarity(v, row))\n",
        "\n",
        "    np.argsort(s)\n",
        "\n",
        "    r = s[-k: ]\n",
        "\n",
        "    return r\n",
        "\n",
        "# UNQ_C8 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "def nearest_neighbor(v, candidates, k=1):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "      - v, the vector you are going find the nearest neighbor for\n",
        "      - candidates: a set of vectors where we will find the neighbors\n",
        "      - k: top k nearest neighbors to find\n",
        "    Output:\n",
        "      - k_idx: the indices of the top k closest vectors in sorted form\n",
        "    \"\"\"\n",
        "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "    similarity_l = []\n",
        "\n",
        "    # for each candidate vector...\n",
        "    for row in candidates:\n",
        "        # get the cosine similarity\n",
        "        cos_similarity = cosine_similarity(v,row)\n",
        "\n",
        "        # append the similarity to the list\n",
        "        similarity_l.append(cos_similarity)\n",
        "        \n",
        "    # sort the similarity list and get the indices of the sorted list\n",
        "    sorted_ids = np.argsort(similarity_l)\n",
        "\n",
        "    # get the indices of the k most similar candidate vectors\n",
        "    k_idx = sorted_ids[-k:]\n",
        "    ### END CODE HERE ###\n",
        "    return k_idx"
      ],
      "metadata": {
        "id": "dDw9oQn7m9Kb"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UNQ_C9 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n",
        "\n",
        "# Test your implementation:\n",
        "v = np.array([1, 0, 1])\n",
        "candidates = np.array([[1, 0, 5], [-2, 5, 3], [2, 0, 1], [6, -9, 5], [9, 9, 9]])\n",
        "print(candidates[nearest_neighbor(v, candidates, 3)])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gj7TDJp2qvO_",
        "outputId": "0e2f29db-6fc2-4176-832d-f804da70fc6d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[9 9 9]\n",
            " [1 0 5]\n",
            " [2 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# UNQ_C10 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "def test_vocabulary(X, Y, R):\n",
        "    '''\n",
        "    Input:\n",
        "        X: a matrix where the columns are the English embeddings.\n",
        "        Y: a matrix where the columns correspong to the French embeddings.\n",
        "        R: the transform matrix which translates word embeddings from\n",
        "        English to French word vector space.\n",
        "    Output:\n",
        "        accuracy: for the English to French capitals\n",
        "    '''\n",
        "\n",
        "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "    # The prediction is X times R\n",
        "    pred = np.dot(X,R)\n",
        "\n",
        "    # initialize the number correct to zero\n",
        "    num_correct = 0\n",
        "\n",
        "    # loop through each row in pred (each transformed embedding)\n",
        "    for i in range(len(pred)):\n",
        "        # get the index of the nearest neighbor of pred at row 'i'; also pass in the candidates in Y\n",
        "        pred_idx = nearest_neighbor(pred[i],Y)\n",
        "\n",
        "        # if the index of the nearest neighbor equals the row of i... \\\n",
        "        if pred_idx == i:\n",
        "            # increment the number correct by 1.\n",
        "            num_correct += 1\n",
        "\n",
        "    # accuracy is the number correct divided by the number of rows in 'pred' (also number of rows in X)\n",
        "    accuracy = num_correct / len(pred)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "0LQbiY39r0Nj"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_val, Y_val = get_matrices(en_fr_test, fr_embeddings_subset, en_embeddings_subset)"
      ],
      "metadata": {
        "id": "MnrSZHfgr-dH"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UNQ_C11 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n",
        "#print((np.dot(X,R)[0]))\n",
        "acc = test_vocabulary(X_val, Y_val, R_train)  # this might take a minute or two\n",
        "print(f\"accuracy on test set is {acc:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlA5uoEusCNU",
        "outputId": "6b0a6c62-92e2-497e-b0df-9d934c4606d6"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy on test set is 0.557\n"
          ]
        }
      ]
    }
  ]
}