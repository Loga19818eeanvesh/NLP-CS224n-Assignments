{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_02.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMjzXZaO287d93AnCSso2f2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Loga19818eeanvesh/Natural_Language_Processing-Assignments/blob/main/Assignment_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YR6TxNdHDzlQ"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "    \"\"\"Compute the softmax function for each row of the input x.\n",
        "    It is crucial that this function is optimized for speed because\n",
        "    it will be used frequently in later code. \n",
        "    Arguments:\n",
        "    x -- A D dimensional vector or N x D dimensional numpy matrix.\n",
        "    Return:\n",
        "    x -- You are allowed to modify x in-place\n",
        "    \"\"\"\n",
        "    orig_shape = x.shape\n",
        "\n",
        "    if len(x.shape) > 1:\n",
        "        # Matrix\n",
        "        tmp = np.max(x, axis=1)\n",
        "        x -= tmp.reshape((x.shape[0], 1))\n",
        "        x = np.exp(x)\n",
        "        tmp = np.sum(x, axis=1)\n",
        "        x /= tmp.reshape((x.shape[0], 1))\n",
        "    else:\n",
        "        # Vector\n",
        "        tmp = np.max(x)\n",
        "        x -= tmp\n",
        "        x = np.exp(x)\n",
        "        tmp = np.sum(x)\n",
        "        x /= tmp\n",
        "\n",
        "    assert x.shape == orig_shape\n",
        "    return x"
      ],
      "metadata": {
        "id": "y3rXaEBREAMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    Compute the sigmoid function for the input here.\n",
        "    Arguments:\n",
        "    x -- A scalar or numpy array.\n",
        "    Return:\n",
        "    s -- sigmoid(x)\n",
        "    \"\"\"\n",
        "\n",
        "    ### YOUR CODE HERE (~1 Line)\n",
        "    s = 1 / (1 + np.exp(-x))\n",
        "    ### END YOUR CODE\n",
        "\n",
        "    return s"
      ],
      "metadata": {
        "id": "oO05dFg_EASc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def naiveSoftmaxLossAndGradient(\n",
        "    centerWordVec,\n",
        "    outsideWordIdx,\n",
        "    outsideVectors,\n",
        "    dataset\n",
        "):\n",
        "    \"\"\" Naive Softmax loss & gradient function for word2vec models\n",
        "    Implement the naive softmax loss and gradients between a center word's \n",
        "    embedding and an outside word's embedding. This will be the building block\n",
        "    for our word2vec models. For those unfamiliar with numpy notation, note \n",
        "    that a numpy ndarray with a shape of (x, ) is a one-dimensional array, which\n",
        "    you can effectively treat as a vector with length x.\n",
        "    Arguments:\n",
        "    centerWordVec -- numpy ndarray, center word's embedding\n",
        "                    in shape (word vector length, )\n",
        "                    (v_c in the pdf handout)\n",
        "    outsideWordIdx -- integer, the index of the outside word\n",
        "                    (o of u_o in the pdf handout)\n",
        "    outsideVectors -- outside vectors is\n",
        "                    in shape (num words in vocab, word vector length) \n",
        "                    for all words in vocab (tranpose of U in the pdf handout)\n",
        "    dataset -- needed for negative sampling, unused here.\n",
        "    Return:\n",
        "    loss -- naive softmax loss\n",
        "    gradCenterVec -- the gradient with respect to the center word vector\n",
        "                     in shape (word vector length, )\n",
        "                     (dJ / dv_c in the pdf handout)\n",
        "    gradOutsideVecs -- the gradient with respect to all the outside word vectors\n",
        "                    in shape (num words in vocab, word vector length) \n",
        "                    (dJ / dU)\n",
        "    \"\"\"\n",
        "\n",
        "    ### YOUR CODE HERE (~6-8 Lines)\n",
        "\n",
        "    ### Please use the provided softmax function (imported earlier in this file)\n",
        "    ### This numerically stable implementation helps you avoid issues pertaining\n",
        "    ### to integer overflow. \n",
        "    \n",
        "    gradOutsideVecs = np.zeros_like(outsideVectors)\n",
        "    # obtain y_hat (i.e., the conditional probability distribution p(O = o | C = c))\n",
        "    # by taking vector dot products and applying softmax\n",
        "    y_hat = softmax(np.dot(outsideVectors, centerWordVec)) # (N,) N x 1\n",
        "    # can also get y_hat in a single line: y_hat = softmax(outsideVectors @ centerWordVec)\n",
        "\n",
        "    # for a single pair of words c and o, the loss is given by:\n",
        "    # J(v_c, o, U) = -log P(O = o | C = c) = -log [y_hat[o]]\n",
        "    loss = -np.log(y_hat[outsideWordIdx])\n",
        "\n",
        "    # grad calc\n",
        "    # generate the ground-truth one-hot vector, [..., 0, outsideWordIdx=1, 0, ...]\n",
        "    y = np.zeros_like(y_hat)\n",
        "    y[outsideWordIdx] = 1\n",
        "    # can also get loss as -np.dot(y, np.log(y_hat))    \n",
        "    \n",
        "    gradCenterVec = np.dot(y_hat - y, outsideVectors) # inner product results in a scalar\n",
        "    # or gradCenterVec = np.dot(outsideVectors.T, y_hat - y)\n",
        "    \n",
        "    gradOutsideVecs = np.dot((y_hat - y)[:, np.newaxis], centerWordVec[np.newaxis, :]) \n",
        "    \n",
        "    # sanity check the dimensions\n",
        "    assert gradCenterVec.shape == centerWordVec.shape\n",
        "    assert gradOutsideVecs.shape == outsideVectors.shape  \n",
        "\n",
        "    ### END YOUR CODE\n",
        "\n",
        "    return loss, gradCenterVec, gradOutsideVecs"
      ],
      "metadata": {
        "id": "B0z7iioXFpcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getNegativeSamples(outsideWordIdx, dataset, K):\n",
        "    \"\"\" Samples K indexes which are not the outsideWordIdx \"\"\"\n",
        "\n",
        "    negSampleWordIndices = [None] * K\n",
        "    for k in range(K):\n",
        "        newidx = dataset.sampleTokenIdx()\n",
        "        while newidx == outsideWordIdx:\n",
        "            newidx = dataset.sampleTokenIdx()\n",
        "        negSampleWordIndices[k] = newidx\n",
        "    return negSampleWordIndices\n"
      ],
      "metadata": {
        "id": "kYX5gk5MGB64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def negSamplingLossAndGradient(\n",
        "    centerWordVec,\n",
        "    outsideWordIdx,\n",
        "    outsideVectors,\n",
        "    dataset,\n",
        "    K=10\n",
        "):\n",
        "    \"\"\" Negative sampling loss function for word2vec models\n",
        "    Implement the negative sampling loss and gradients for a centerWordVec\n",
        "    and a outsideWordIdx word vector as a building block for word2vec\n",
        "    models. K is the number of negative samples to take.\n",
        "    Note: The same word may be negatively sampled multiple times. For\n",
        "    example if an outside word is sampled twice, you shall have to\n",
        "    double count the gradient with respect to this word. Thrice if\n",
        "    it was sampled three times, and so forth.\n",
        "    Arguments/Return Specifications: same as naiveSoftmaxLossAndGradient\n",
        "    \"\"\"\n",
        "\n",
        "    # Negative sampling of words is done for you. Do not modify this if you\n",
        "    # wish to match the autograder and receive points!\n",
        "    negSampleWordIndices = getNegativeSamples(outsideWordIdx, dataset, K)\n",
        "    indices = [outsideWordIdx] + negSampleWordIndices\n",
        "\n",
        "    ### YOUR CODE HERE (~10 Lines)\n",
        "\n",
        "    ### Please use your implementation of sigmoid in here.\n",
        "    \n",
        "    gradOutsideVecs = np.zeros(outsideVectors.shape)\n",
        "    \n",
        "    # Calculate the first term\n",
        "    y_hat = sigmoid(np.dot(outsideVectors[outsideWordIdx], centerWordVec))\n",
        "    loss = -np.log(y_hat)\n",
        "    \n",
        "    gradCenterVec = np.dot(y_hat - 1, outsideVectors[outsideWordIdx])\n",
        "    gradOutsideVecs[outsideWordIdx] = np.dot(y_hat - 1, centerWordVec)\n",
        "\n",
        "    # Calculate the second term\n",
        "    for i in range(K):\n",
        "        w_k = indices[i+1]\n",
        "        y_k_hat = sigmoid(-np.dot(outsideVectors[w_k], centerWordVec))\n",
        "        loss += -np.log(y_k_hat)\n",
        "        gradOutsideVecs[w_k] += np.dot(1.0 - y_k_hat, centerWordVec)\n",
        "        gradCenterVec += np.dot(1.0 - y_k_hat, outsideVectors[w_k])\n",
        "\n",
        "    ### END YOUR CODE\n",
        "\n",
        "    return loss, gradCenterVec, gradOutsideVecs"
      ],
      "metadata": {
        "id": "7q5s8MutGiug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def skipgram(currentCenterWord, windowSize, outsideWords, word2Ind,\n",
        "             centerWordVectors, outsideVectors, dataset,\n",
        "             word2vecLossAndGradient=naiveSoftmaxLossAndGradient):\n",
        "    \"\"\" Skip-gram model in word2vec\n",
        "    Implement the skip-gram model in this function.\n",
        "    Arguments:\n",
        "    currentCenterWord -- a string of the current center word\n",
        "    windowSize -- integer, context window size\n",
        "    outsideWords -- list of no more than 2*windowSize strings, the outside words\n",
        "    word2Ind -- a dictionary that maps words to their indices in\n",
        "              the word vector list\n",
        "    centerWordVectors -- center word vectors (as rows) is in shape \n",
        "                        (num words in vocab, word vector length) \n",
        "                        for all words in vocab (V in pdf handout)\n",
        "    outsideVectors -- outside vectors is in shape \n",
        "                        (num words in vocab, word vector length) \n",
        "                        for all words in vocab (transpose of U in the pdf handout)\n",
        "    word2vecLossAndGradient -- the loss and gradient function for\n",
        "                               a prediction vector given the outsideWordIdx\n",
        "                               word vectors, could be one of the two\n",
        "                               loss functions you implemented above.\n",
        "    Return:\n",
        "    loss -- the loss function value for the skip-gram model\n",
        "            (J in the pdf handout)\n",
        "    gradCenterVec -- the gradient with respect to the center word vector\n",
        "                     in shape (word vector length, )\n",
        "                     (dJ / dv_c in the pdf handout)\n",
        "    gradOutsideVecs -- the gradient with respect to all the outside word vectors\n",
        "                    in shape (num words in vocab, word vector length) \n",
        "                    (dJ / dU)\n",
        "    \"\"\"\n",
        "\n",
        "    loss = 0.0\n",
        "    gradCenterVecs = np.zeros(centerWordVectors.shape)\n",
        "    gradOutsideVectors = np.zeros(outsideVectors.shape)\n",
        "\n",
        "    ### YOUR CODE HERE (~8 Lines)\n",
        "    \n",
        "    # skip-gram model predicts outside words from the center word\n",
        "    \n",
        "    # get center word vec first from currentCenterWord\n",
        "    centerWordIdx = word2Ind[currentCenterWord]\n",
        "    centerWordVec = centerWordVectors[centerWordIdx]\n",
        "\n",
        "    for outsideWord in outsideWords:\n",
        "        outsideWordIdx = word2Ind[outsideWord]\n",
        "        stepLoss, gradCenter, gradOutside = word2vecLossAndGradient(centerWordVec,\n",
        "                                                                    outsideWordIdx,\n",
        "                                                                    outsideVectors,\n",
        "                                                                    dataset)\n",
        "\n",
        "        loss += stepLoss\n",
        "        gradCenterVecs[centerWordIdx] += gradCenter\n",
        "        gradOutsideVectors += gradOutside    \n",
        "\n",
        "    ### END YOUR CODE\n",
        "    \n",
        "    return loss, gradCenterVecs, gradOutsideVectors\n"
      ],
      "metadata": {
        "id": "_5lt7yFBJ43y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "\n",
        "# Save parameters every a few SGD iterations as fail-safe\n",
        "SAVE_PARAMS_EVERY = 5000\n",
        "\n",
        "import pickle\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import os.path as op\n",
        "\n",
        "def load_saved_params():\n",
        "    \"\"\"\n",
        "    A helper function that loads previously saved parameters and resets\n",
        "    iteration start.\n",
        "    \"\"\"\n",
        "    st = 0\n",
        "    for f in glob.glob(\"saved_params_*.npy\"):\n",
        "        iter = int(op.splitext(op.basename(f))[0].split(\"_\")[2])\n",
        "        if (iter > st):\n",
        "            st = iter\n",
        "\n",
        "    if st > 0:\n",
        "        params_file = \"saved_params_%d.npy\" % st\n",
        "        state_file = \"saved_state_%d.pickle\" % st\n",
        "        params = np.load(params_file)\n",
        "        with open(state_file, \"rb\") as f:\n",
        "            state = pickle.load(f)\n",
        "        return st, params, state\n",
        "    else:\n",
        "        return st, None, None\n",
        "\n",
        "\n",
        "def save_params(iter, params):\n",
        "    params_file = \"saved_params_%d.npy\" % iter\n",
        "    np.save(params_file, params)\n",
        "    with open(\"saved_state_%d.pickle\" % iter, \"wb\") as f:\n",
        "        pickle.dump(random.getstate(), f)\n",
        "\n",
        "\n",
        "def sgd(f, x0, step, iterations, postprocessing=None, useSaved=False,\n",
        "        PRINT_EVERY=10):\n",
        "    \"\"\" Stochastic Gradient Descent\n",
        "    Implement the stochastic gradient descent method in this function.\n",
        "    Arguments:\n",
        "    f -- the function to optimize, it should take a single\n",
        "         argument and yield two outputs, a loss and the gradient\n",
        "         with respect to the arguments\n",
        "    x0 -- the initial point to start SGD from\n",
        "    step -- the step size for SGD\n",
        "    iterations -- total iterations to run SGD for\n",
        "    postprocessing -- postprocessing function for the parameters\n",
        "                      if necessary. In the case of word2vec we will need to\n",
        "                      normalize the word vectors to have unit length.\n",
        "    PRINT_EVERY -- specifies how many iterations to output loss\n",
        "    Return:\n",
        "    x -- the parameter value after SGD finishes\n",
        "    \"\"\"\n",
        "\n",
        "    # Anneal learning rate every several iterations\n",
        "    ANNEAL_EVERY = 20000\n",
        "\n",
        "    if useSaved:\n",
        "        start_iter, oldx, state = load_saved_params()\n",
        "        if start_iter > 0:\n",
        "            x0 = oldx\n",
        "            step *= 0.5 ** (start_iter / ANNEAL_EVERY)\n",
        "\n",
        "        if state:\n",
        "            random.setstate(state)\n",
        "    else:\n",
        "        start_iter = 0\n",
        "\n",
        "    x = x0\n",
        "\n",
        "    if not postprocessing:\n",
        "        postprocessing = lambda x: x\n",
        "\n",
        "    exploss = None\n",
        "\n",
        "    for iter in range(start_iter + 1, iterations + 1):\n",
        "        # You might want to print the progress every few iterations.\n",
        "\n",
        "        loss = None\n",
        "        ### YOUR CODE HERE (~2 lines)\n",
        "        \n",
        "        # Calculate loss and gradients\n",
        "        loss, grad = f(x)\n",
        "        \n",
        "        # Take step in direction of gradient.\n",
        "        x -= step * grad\n",
        "        ### END YOUR CODE\n",
        "\n",
        "        x = postprocessing(x)\n",
        "        if iter % PRINT_EVERY == 0:\n",
        "            if not exploss:\n",
        "                exploss = loss\n",
        "            else:\n",
        "                exploss = .95 * exploss + .05 * loss\n",
        "            print(\"iter %d: %f\" % (iter, exploss))\n",
        "\n",
        "        if iter % SAVE_PARAMS_EVERY == 0 and useSaved:\n",
        "            save_params(iter, x)\n",
        "\n",
        "        if iter % ANNEAL_EVERY == 0:\n",
        "            step *= 0.5\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def sanity_check():\n",
        "    quad = lambda x: (np.sum(x ** 2), x * 2)\n",
        "\n",
        "    print(\"Running sanity checks...\")\n",
        "    t1 = sgd(quad, 0.5, 0.01, 1000, PRINT_EVERY=100)\n",
        "    print(\"test 1 result:\", t1)\n",
        "    assert abs(t1) <= 1e-6\n",
        "\n",
        "    t2 = sgd(quad, 0.0, 0.01, 1000, PRINT_EVERY=100)\n",
        "    print(\"test 2 result:\", t2)\n",
        "    assert abs(t2) <= 1e-6\n",
        "\n",
        "    t3 = sgd(quad, -1.5, 0.01, 1000, PRINT_EVERY=100)\n",
        "    print(\"test 3 result:\", t3)\n",
        "    assert abs(t3) <= 1e-6\n",
        "\n",
        "    print(\"-\" * 40)\n",
        "    print(\"ALL TESTS PASSED\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    sanity_check()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1pjsVLdYQHO",
        "outputId": "1b9d5404-c897-4ef8-e3a8-751fb84fb46d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running sanity checks...\n",
            "iter 100: 0.004578\n",
            "iter 200: 0.004353\n",
            "iter 300: 0.004136\n",
            "iter 400: 0.003929\n",
            "iter 500: 0.003733\n",
            "iter 600: 0.003546\n",
            "iter 700: 0.003369\n",
            "iter 800: 0.003200\n",
            "iter 900: 0.003040\n",
            "iter 1000: 0.002888\n",
            "test 1 result: 8.414836786079764e-10\n",
            "iter 100: 0.000000\n",
            "iter 200: 0.000000\n",
            "iter 300: 0.000000\n",
            "iter 400: 0.000000\n",
            "iter 500: 0.000000\n",
            "iter 600: 0.000000\n",
            "iter 700: 0.000000\n",
            "iter 800: 0.000000\n",
            "iter 900: 0.000000\n",
            "iter 1000: 0.000000\n",
            "test 2 result: 0.0\n",
            "iter 100: 0.041205\n",
            "iter 200: 0.039181\n",
            "iter 300: 0.037222\n",
            "iter 400: 0.035361\n",
            "iter 500: 0.033593\n",
            "iter 600: 0.031913\n",
            "iter 700: 0.030318\n",
            "iter 800: 0.028802\n",
            "iter 900: 0.027362\n",
            "iter 1000: 0.025994\n",
            "test 3 result: -2.524451035823933e-09\n",
            "----------------------------------------\n",
            "ALL TESTS PASSED\n",
            "----------------------------------------\n"
          ]
        }
      ]
    }
  ]
}