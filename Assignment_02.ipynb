{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_02.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNfCvcKrvjDfahbIgLyqIzA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Loga19818eeanvesh/Natural_Language_Processing-Assignments/blob/main/Assignment_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "YR6TxNdHDzlQ"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "    \"\"\"Compute the softmax function for each row of the input x.\n",
        "    It is crucial that this function is optimized for speed because\n",
        "    it will be used frequently in later code. \n",
        "    Arguments:\n",
        "    x -- A D dimensional vector or N x D dimensional numpy matrix.\n",
        "    Return:\n",
        "    x -- You are allowed to modify x in-place\n",
        "    \"\"\"\n",
        "    orig_shape = x.shape\n",
        "\n",
        "    if len(x.shape) > 1:\n",
        "        # Matrix\n",
        "        tmp = np.max(x, axis=1)\n",
        "        x -= tmp.reshape((x.shape[0], 1))\n",
        "        x = np.exp(x)\n",
        "        tmp = np.sum(x, axis=1)\n",
        "        x /= tmp.reshape((x.shape[0], 1))\n",
        "    else:\n",
        "        # Vector\n",
        "        tmp = np.max(x)\n",
        "        x -= tmp\n",
        "        x = np.exp(x)\n",
        "        tmp = np.sum(x)\n",
        "        x /= tmp\n",
        "\n",
        "    assert x.shape == orig_shape\n",
        "    return x"
      ],
      "metadata": {
        "id": "y3rXaEBREAMi"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    Compute the sigmoid function for the input here.\n",
        "    Arguments:\n",
        "    x -- A scalar or numpy array.\n",
        "    Return:\n",
        "    s -- sigmoid(x)\n",
        "    \"\"\"\n",
        "\n",
        "    ### YOUR CODE HERE (~1 Line)\n",
        "    s = 1 / (1 + np.exp(-x))\n",
        "    ### END YOUR CODE\n",
        "\n",
        "    return s"
      ],
      "metadata": {
        "id": "oO05dFg_EASc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def naiveSoftmaxLossAndGradient(\n",
        "    centerWordVec,\n",
        "    outsideWordIdx,\n",
        "    outsideVectors,\n",
        "    dataset\n",
        "):\n",
        "    \"\"\" Naive Softmax loss & gradient function for word2vec models\n",
        "    Implement the naive softmax loss and gradients between a center word's \n",
        "    embedding and an outside word's embedding. This will be the building block\n",
        "    for our word2vec models. For those unfamiliar with numpy notation, note \n",
        "    that a numpy ndarray with a shape of (x, ) is a one-dimensional array, which\n",
        "    you can effectively treat as a vector with length x.\n",
        "    Arguments:\n",
        "    centerWordVec -- numpy ndarray, center word's embedding\n",
        "                    in shape (word vector length, )\n",
        "                    (v_c in the pdf handout)\n",
        "    outsideWordIdx -- integer, the index of the outside word\n",
        "                    (o of u_o in the pdf handout)\n",
        "    outsideVectors -- outside vectors is\n",
        "                    in shape (num words in vocab, word vector length) \n",
        "                    for all words in vocab (tranpose of U in the pdf handout)\n",
        "    dataset -- needed for negative sampling, unused here.\n",
        "    Return:\n",
        "    loss -- naive softmax loss\n",
        "    gradCenterVec -- the gradient with respect to the center word vector\n",
        "                     in shape (word vector length, )\n",
        "                     (dJ / dv_c in the pdf handout)\n",
        "    gradOutsideVecs -- the gradient with respect to all the outside word vectors\n",
        "                    in shape (num words in vocab, word vector length) \n",
        "                    (dJ / dU)\n",
        "    \"\"\"\n",
        "\n",
        "    ### YOUR CODE HERE (~6-8 Lines)\n",
        "\n",
        "    ### Please use the provided softmax function (imported earlier in this file)\n",
        "    ### This numerically stable implementation helps you avoid issues pertaining\n",
        "    ### to integer overflow. \n",
        "    \n",
        "    gradOutsideVecs = np.zeros_like(outsideVectors)\n",
        "    # obtain y_hat (i.e., the conditional probability distribution p(O = o | C = c))\n",
        "    # by taking vector dot products and applying softmax\n",
        "    y_hat = softmax(np.dot(outsideVectors, centerWordVec)) # (N,) N x 1\n",
        "    # can also get y_hat in a single line: y_hat = softmax(outsideVectors @ centerWordVec)\n",
        "\n",
        "    # for a single pair of words c and o, the loss is given by:\n",
        "    # J(v_c, o, U) = -log P(O = o | C = c) = -log [y_hat[o]]\n",
        "    loss = -np.log(y_hat[outsideWordIdx])\n",
        "\n",
        "    # grad calc\n",
        "    # generate the ground-truth one-hot vector, [..., 0, outsideWordIdx=1, 0, ...]\n",
        "    y = np.zeros_like(y_hat)\n",
        "    y[outsideWordIdx] = 1\n",
        "    # can also get loss as -np.dot(y, np.log(y_hat))    \n",
        "    \n",
        "    gradCenterVec = np.dot(y_hat - y, outsideVectors) # inner product results in a scalar\n",
        "    # or gradCenterVec = np.dot(outsideVectors.T, y_hat - y)\n",
        "    \n",
        "    gradOutsideVecs = np.dot((y_hat - y)[:, np.newaxis], centerWordVec[np.newaxis, :]) \n",
        "    \n",
        "    # sanity check the dimensions\n",
        "    assert gradCenterVec.shape == centerWordVec.shape\n",
        "    assert gradOutsideVecs.shape == outsideVectors.shape  \n",
        "\n",
        "    ### END YOUR CODE\n",
        "\n",
        "    return loss, gradCenterVec, gradOutsideVecs"
      ],
      "metadata": {
        "id": "B0z7iioXFpcS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getNegativeSamples(outsideWordIdx, dataset, K):\n",
        "    \"\"\" Samples K indexes which are not the outsideWordIdx \"\"\"\n",
        "\n",
        "    negSampleWordIndices = [None] * K\n",
        "    for k in range(K):\n",
        "        newidx = dataset.sampleTokenIdx()\n",
        "        while newidx == outsideWordIdx:\n",
        "            newidx = dataset.sampleTokenIdx()\n",
        "        negSampleWordIndices[k] = newidx\n",
        "    return negSampleWordIndices\n"
      ],
      "metadata": {
        "id": "kYX5gk5MGB64"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def negSamplingLossAndGradient(\n",
        "    centerWordVec,\n",
        "    outsideWordIdx,\n",
        "    outsideVectors,\n",
        "    dataset,\n",
        "    K=10\n",
        "):\n",
        "    \"\"\" Negative sampling loss function for word2vec models\n",
        "    Implement the negative sampling loss and gradients for a centerWordVec\n",
        "    and a outsideWordIdx word vector as a building block for word2vec\n",
        "    models. K is the number of negative samples to take.\n",
        "    Note: The same word may be negatively sampled multiple times. For\n",
        "    example if an outside word is sampled twice, you shall have to\n",
        "    double count the gradient with respect to this word. Thrice if\n",
        "    it was sampled three times, and so forth.\n",
        "    Arguments/Return Specifications: same as naiveSoftmaxLossAndGradient\n",
        "    \"\"\"\n",
        "\n",
        "    # Negative sampling of words is done for you. Do not modify this if you\n",
        "    # wish to match the autograder and receive points!\n",
        "    negSampleWordIndices = getNegativeSamples(outsideWordIdx, dataset, K)\n",
        "    indices = [outsideWordIdx] + negSampleWordIndices\n",
        "\n",
        "    ### YOUR CODE HERE (~10 Lines)\n",
        "\n",
        "    ### Please use your implementation of sigmoid in here.\n",
        "    \n",
        "    gradOutsideVecs = np.zeros(outsideVectors.shape)\n",
        "    \n",
        "    # Calculate the first term\n",
        "    y_hat = sigmoid(np.dot(outsideVectors[outsideWordIdx], centerWordVec))\n",
        "    loss = -np.log(y_hat)\n",
        "    \n",
        "    gradCenterVec = np.dot(y_hat - 1, outsideVectors[outsideWordIdx])\n",
        "    gradOutsideVecs[outsideWordIdx] = np.dot(y_hat - 1, centerWordVec)\n",
        "\n",
        "    # Calculate the second term\n",
        "    for i in range(K):\n",
        "        w_k = indices[i+1]\n",
        "        y_k_hat = sigmoid(-np.dot(outsideVectors[w_k], centerWordVec))\n",
        "        loss += -np.log(y_k_hat)\n",
        "        gradOutsideVecs[w_k] += np.dot(1.0 - y_k_hat, centerWordVec)\n",
        "        gradCenterVec += np.dot(1.0 - y_k_hat, outsideVectors[w_k])\n",
        "\n",
        "    ### END YOUR CODE\n",
        "\n",
        "    return loss, gradCenterVec, gradOutsideVecs"
      ],
      "metadata": {
        "id": "7q5s8MutGiug"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def skipgram(currentCenterWord, windowSize, outsideWords, word2Ind,\n",
        "             centerWordVectors, outsideVectors, dataset,\n",
        "             word2vecLossAndGradient=naiveSoftmaxLossAndGradient):\n",
        "    \"\"\" Skip-gram model in word2vec\n",
        "    Implement the skip-gram model in this function.\n",
        "    Arguments:\n",
        "    currentCenterWord -- a string of the current center word\n",
        "    windowSize -- integer, context window size\n",
        "    outsideWords -- list of no more than 2*windowSize strings, the outside words\n",
        "    word2Ind -- a dictionary that maps words to their indices in\n",
        "              the word vector list\n",
        "    centerWordVectors -- center word vectors (as rows) is in shape \n",
        "                        (num words in vocab, word vector length) \n",
        "                        for all words in vocab (V in pdf handout)\n",
        "    outsideVectors -- outside vectors is in shape \n",
        "                        (num words in vocab, word vector length) \n",
        "                        for all words in vocab (transpose of U in the pdf handout)\n",
        "    word2vecLossAndGradient -- the loss and gradient function for\n",
        "                               a prediction vector given the outsideWordIdx\n",
        "                               word vectors, could be one of the two\n",
        "                               loss functions you implemented above.\n",
        "    Return:\n",
        "    loss -- the loss function value for the skip-gram model\n",
        "            (J in the pdf handout)\n",
        "    gradCenterVec -- the gradient with respect to the center word vector\n",
        "                     in shape (word vector length, )\n",
        "                     (dJ / dv_c in the pdf handout)\n",
        "    gradOutsideVecs -- the gradient with respect to all the outside word vectors\n",
        "                    in shape (num words in vocab, word vector length) \n",
        "                    (dJ / dU)\n",
        "    \"\"\"\n",
        "\n",
        "    loss = 0.0\n",
        "    gradCenterVecs = np.zeros(centerWordVectors.shape)\n",
        "    gradOutsideVectors = np.zeros(outsideVectors.shape)\n",
        "\n",
        "    ### YOUR CODE HERE (~8 Lines)\n",
        "    \n",
        "    # skip-gram model predicts outside words from the center word\n",
        "    \n",
        "    # get center word vec first from currentCenterWord\n",
        "    centerWordIdx = word2Ind[currentCenterWord]\n",
        "    centerWordVec = centerWordVectors[centerWordIdx]\n",
        "\n",
        "    for outsideWord in outsideWords:\n",
        "        outsideWordIdx = word2Ind[outsideWord]\n",
        "        stepLoss, gradCenter, gradOutside = word2vecLossAndGradient(centerWordVec,\n",
        "                                                                    outsideWordIdx,\n",
        "                                                                    outsideVectors,\n",
        "                                                                    dataset)\n",
        "\n",
        "        loss += stepLoss\n",
        "        gradCenterVecs[centerWordIdx] += gradCenter\n",
        "        gradOutsideVectors += gradOutside    \n",
        "\n",
        "    ### END YOUR CODE\n",
        "    \n",
        "    return loss, gradCenterVecs, gradOutsideVectors\n"
      ],
      "metadata": {
        "id": "_5lt7yFBJ43y"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}